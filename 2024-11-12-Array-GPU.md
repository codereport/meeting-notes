### Links

* https://mlir.llvm.org/docs/Dialects/Linalg/
* https://mlir.llvm.org/docs/Dialects/TensorOps/
* https://github.com/llvm/llvm-project/blob/main/flang/docs/ArrayComposition.md
* https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=push+pull+arrays&btnG=
* https://dl.acm.org/doi/pdf/10.1145/268946.268956

> JAX achieves fusion of NumPy-like operations by using a technique called just-in-time (JIT) compilation, in combination with tracing and XLA (Accelerated Linear Algebra), a compiler backend developed by Google. Here’s how it works:
> Tracing and Computation Graph Construction: When you write code in JAX, it “traces” your computation using a functional, side-effect-free model, where operations are tracked to construct a computation graph. Instead of immediately evaluating each operation, JAX captures the sequence of operations, which allows it to see the entire chain of operations as a single entity.
> JIT Compilation with XLA: Once the computation graph is built, JAX can pass this graph to XLA. XLA then optimizes and compiles the graph into efficient machine code, specifically targeting the underlying hardware (CPU, GPU, or TPU). XLA’s optimization includes operation fusion, where it combines multiple operations into a single kernel that can run without intermediate memory allocations, reducing the overhead of transferring data between operations.
> Automatic Operation Fusion: XLA’s fusion process works by analyzing operations in the computation graph that can be merged together, such as element-wise operations (e.g., addition, multiplication, etc.) on arrays. Instead of performing each operation separately and storing intermediate results in memory, XLA generates code that combines these operations into a single fused kernel, executed in a single pass over the data.
> Efficient Memory and Execution Management: By fusing operations, JAX and XLA reduce memory usage and improve cache locality, as the fused kernel minimizes data transfers and keeps relevant data in cache. This leads to faster execution, as JAX minimizes intermediate data storage and maximizes hardware efficiency.
> Overall, JAX manages to “fuse” NumPy operations by capturing them into a graph, optimizing that graph with XLA, and leveraging XLA’s capability to fuse and compile efficient kernels for the entire computation. This process is key to achieving high performance, especially in GPU and TPU environments where JAX is commonly used.

```python
# jax 
# type: ignore

import numpy as np
from scipy.stats import norm


def test_call_option():

    def call_option(S, K, T, r, V):  # noqa: N803
        s = V * np.sqrt(T)
        d1 = (np.log(S / K) + (r + 0.5 * V**2) * T) / s
        d2 = d1 - s
        x = np.exp(-r * T)
        return S * norm.cdf(d1) - K * x * norm.cdf(d2)

    s = np.array([90, 100, 110])
    result = call_option(s, 100, 1, 0.05, 0.1)
    expected = np.array([1.681, 6.805, 15.210])

    np.testing.assert_almost_equal(result, expected, decimal=3)
```
